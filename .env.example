# Iris - AI Provider Configuration
# Copy this file to .env and add your API keys

# ============================================
# AI Provider API Keys (All Optional)
# ============================================

# OpenAI (GPT-4o, o1-preview for advanced reasoning)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Groq (Ultra-fast inference with Llama/Mixtral)
GROQ_API_KEY=gsk_your-groq-api-key-here

# Google Gemini (Multimodal analysis)
GEMINI_API_KEY=AI-your-gemini-api-key-here

# Anthropic Claude (Advanced reasoning)
ANTHROPIC_API_KEY=sk-ant-your-claude-api-key-here

# ============================================
# Local AI Configuration
# ============================================

# Ollama server host (local AI - FREE)
OLLAMA_HOST=http://localhost:11434

# ============================================
# Performance Configuration
# ============================================

# Cache settings
CACHE_ENABLED=true
CACHE_TTL_HOURS=24
MAX_CACHE_SIZE=10000

# Request timeouts (milliseconds)
DEFAULT_TIMEOUT=30000
GROQ_TIMEOUT=15000
OPENAI_TIMEOUT=60000

# ============================================
# Development Settings
# ============================================

# Enable verbose logging
DEBUG=false
LOG_LEVEL=info

# Performance monitoring
PERFORMANCE_MONITORING=true
ANALYTICS_ENABLED=true

# ============================================
# Notes
# ============================================
# 
# - Ollama works without any API keys (100% free, local)
# - API keys are optional but enable advanced features
# - Groq offers generous free tier for fast inference
# - OpenAI required for o1-preview and GPT-4o models
# - Gemini great for multimodal analysis
# - Claude excellent for general reasoning tasks
#
# Get API keys:
# - OpenAI: https://platform.openai.com/api-keys
# - Groq: https://console.groq.com/keys
# - Gemini: https://aistudio.google.com/app/apikey
# - Claude: https://console.anthropic.com/